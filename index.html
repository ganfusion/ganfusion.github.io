<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="GANFusion enables text prompting for feed-forward 3D generators that are trained with single-view supervision.">
  <meta name="keywords" content="Diffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Diffusion Handles</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<style>
  table { 
      table-layout:fixed;
      width:100%;
  }
  img {
      max-height:100%;
      max-width:100%;
  }
  thead th {
      top: 0;
      position: sticky;
      z-index: 20;
      background-color: white;
  }
  </style>
<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
        </a>
  
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            Full Results
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="./static/full-results/qualitative_comparison_main.html">
              Qualitative Comparisons on PhotoGen 
            </a>
            <a class="navbar-item" href="./static/full-results/qualitative_comparison_benchmark.html">
              Qualitative Comparisons on Benchmark
            </a>
          </div>
        </div>
      </div>
  
    </div>
  </nav> -->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="dnerf">GANFusion: <br> Feed-Forward Text-to-3D with Diffusion in GAN Space</span>  </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=REUg_ToAAAAJ">Souhaib Attaiki</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://paulguerrero.net/">Paul Guerrero</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.duygu-ceylan.com/">Duygu Ceylan</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/">Niloy Mitra</a><sup>2,3</sup>
            </span>
            <span class="author-block">
              <a href="https://www.lix.polytechnique.fr/~maks/">Maks Ovsjanikov</a><sup>1</sup>,
            </span>

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Ecole Polytechnique,</span>
            <span class="author-block"><sup>2</sup>Adobe Research,</span>
            <span class="author-block"><sup>3</sup>University College London</span>

          </div>

          <div class="is-size-5 publication-venue">
          WACV 2025
          </div>

         <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Arxiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
            </div>

          </div> 
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4" type="video/mp4">
      </video> -->
      <img src="./static/images/teaser.png"
      class = "interpolation-image"
        alt="Text-to-3D on FFHQ."/>
        <img src="./static/images/teaser2.png"
        class = "interpolation-image"
          alt="Teaser."/>
        <h2 class="subtitle has-text-centered">
        <span class="dnerf">GANFusion</span> is a text-guided feed-forward 3D generator that is trained with only single-view image supervision. Unlike
        previous methods such as <a herf="https://zj-dong.github.io/AG3D/">AG3D</a>, which do not enable text conditioning, GANFusion can be conditioned on text while still achieving
        high generation quality compared to other text conditioned generators such as <a href="https://github.com/Anciukevicius/RenderDiffusion">RenderDiffusion</a>. We note that GANFusion, unlike
        <a href="https://dreamfusion3d.github.io/">SDS-based optimization methods</a> that use image diffusion priors, does not require any test-time optimization.
      </h2>
    </div>
  </div>
</section>

<!-- Abstract. -->
<section class="section" style="background-color:#f7f7f7">
  <div class="container is-max-desktop" >
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We train a feed-forward text-to-3D diffusion generator for human characters using only single-view 2D data for supervision.
            Existing 3D generative models cannot yet match the fidelity of image and/or video generative models.
            Stateof-the-art 3D generators are either trained with explicit 3D supervision and are thus limited by the volume and diversity
            of existing 3D data.
            Meanwhile, generators that can be trained with only 2D data as supervision typically produce coarser results,
            cannot be text-conditioned, and/or must revert to test-time optimization.
            We observe that GAN- and diffusion-based generators have complementary qualities:
            GANs can be trained efficiently with 2D supervision to produce high-quality 3D objects but are hard to condition on
            text. In contrast, denoising diffusion models can be conditioned efficiently but tend to be hard to train with only 2D supervision.
            We introduce <span class="dnerf">GANFusion</span> that starts by generating unconditional triplane features for 3D data using a
            GAN architecture trained with only single-view 2D data.
            We then generate random samples from the GAN, caption them, and train a text-conditioned diffusion model that directly
            learns to sample from the space of good triplane features that can be decoded into 3D objects.
            We evaluate the proposed method in the context of text-conditioned
            full-body human generation and show improvements over possible alternatives.
          </p>
        </div>
      </div>
    </div>

    <!-- Paper video. -->
   <!--<div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<!-- Results. -->
<!-- <section class="section">
  <div class="container is-max-desktop is-centered has-text-centered">
        
    <h2 class="title is-3">3D-Aware Editing Results</h2> -->

    <!-- Main Results. -->
    <!-- <div id="results-carousel" class="carousel results-carousel">
      <div class="item d-flex justify-content-center">
        <img src="./static/result-images/toaster_screen.png"
        class = "interpolation-image"
          alt="3D-aware object edits for diffusion images."/>
      </div>
      <div class="item d-flex justify-content-center">
        <img src="./static/result-images/shoe.png"
        class = "interpolation-image"
          alt="3D-aware object edits for diffusion images."/>
      </div>
      <div class="item d-flex justify-content-center">
        <img src="./static/result-images/potted_plant_screen.png"
        class = "interpolation-image"
          alt="3D-aware object edits for diffusion images."/>
      </div>
      <div class="item d-flex justify-content-center">
        <img src="./static/result-images/wine_screen.png"
        class = "interpolation-image"
          alt="3D-aware object edits for diffusion images."/>
      </div>
      <div class="item d-flex justify-content-center">
        <img src="./static/result-images/real_car.png"
        class = "interpolation-image"
          alt="3D-aware object edits for diffusion images."/>
      </div>
      <div class="item d-flex justify-content-center">
        <img src="./static/result-images/plane_screen.png"
        class = "interpolation-image"
          alt="3D-aware object edits for diffusion images."/>
      </div>
      <div class="item d-flex justify-content-center">
        <img src="./static/result-images/guitar_screen.png"
        class = "interpolation-image"
          alt="3D-aware object edits for diffusion images."/>
      </div>
      <div class="item d-flex justify-content-center">
        <img src="./static/result-images/building_screen.png"
        class = "interpolation-image"
          alt="3D-aware object edits for diffusion images."/>
      </div>
    </div> -->

    <!-- Comparisons. -->
    <!-- <br/>
    <h2 class="title is-3">Comparison to the State-of-the-Art</h2>
    <table>
      <thead>
          <tr>
              <th>Input</th>
              <th>Target Edit</th>
              <th>Ours</th>
              <th>Obj. Stitch</th>
              <th>Zero123</th>
              <th>3DIT</th>
          </tr>
      </thead>
      <tbody>
          
          <tr>
              <td><img src="static/full-results/images/qualitative_comparison_main/sunflower_2_input.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/sunflower_2_edit.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/sunflower_2_ours.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/sunflower_2_objstitch.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/sunflower_2_zero123.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/sunflower_2_3dit.png"></td>
          </tr>
          
          <tr>
              <td><img src="static/full-results/images/qualitative_comparison_main/outputorig_jet_input.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/outputorig_jet_edit.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/outputorig_jet_ours.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/outputorig_jet_objstitch.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/outputorig_jet_zero123.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/outputorig_jet_3dit.png"></td>
          </tr>
          
          <tr>
              <td><img src="static/full-results/images/qualitative_comparison_main/outputorig_red_car_input.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/outputorig_red_car_edit.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/outputorig_red_car_ours.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/outputorig_red_car_objstitch.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/outputorig_red_car_zero123.png"></td>
              <td><img src="static/full-results/images/qualitative_comparison_main/outputorig_red_car_3dit.png"></td>
          </tr>
      </tbody>
    </table>
    <p>(Comparison to the state-of-the-art on a much larger set of images are available here for images with <a href="./static/full-results/qualitative_comparison_main.html">estimated depth</a> & <a href="./static/full-results/qualitative_comparison_benchmark.html">synthetic depth</a>.)
    </p>
    <br/>
    <p style="text-align: left; font-size: smaller;">
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_ObjectStitch_Object_Compositing_With_Diffusion_Model_CVPR_2023_paper.pdf">ObjectStitch</a> is a generative object compositing method which we adapt for object-level editing on scenes.
      <br/>
      <a href="https://zero123.cs.columbia.edu/">Zero-1-2-3</a> is a diffusion-based novel-view synthesis method which we use in conjunction with inpainting to perform object-level 3D-aware image editing.
      <br/>
      <a href="https://prior.allenai.org/projects/object-edit">Object 3DIT</a> is a language-guided model for 3D-aware image editing.
    </p>
    <br/> -->

    
    <!-- Interpolation 1. -->
    <!-- <br/>
    <h2 class="title is-3">Frame-by-Frame Interpolation</h2>
    <div class="columns is-vcentered interpolation-panel">
      <div class="column is-3 has-text-centered">
        <img src="./static/interpolated-car/0.png"
              class="interpolation-image"
              alt="Interpolate start reference image."/>
        <p>Start Frame</p>
      </div>
      <div class="column interpolation-video-column">
        <div id="interpolation-image-wrapper">
          Loading...
        </div>
        <input class="slider is-fullwidth is-large is-info"
                id="interpolation-slider"
                step="1" min="0" max="100" value="0" type="range">
      </div>
      <div class="column is-3 has-text-centered">
        <img src="./static/interpolated-car/4.png"
              class="interpolation-image"
              alt="Interpolation end reference image."/>
        <p class="is-bold">End Frame</p>
      </div>
    </div> -->
    
    <!-- Interpolation 2. -->
    <!-- <br/>
    <div class="columns is-vcentered interpolation-panel">
      <div class="column is-3 has-text-centered">
        <img src="./static/interpolated-sunflower/0.png"
              class="interpolation-image"
              alt="Interpolate start reference image."/>
        <p>Start Frame</p>
      </div>
      <div class="column interpolation-video-column">
        <div id="interpolation-image-wrapper-2">
          Loading...
        </div>
        <input class="slider is-fullwidth is-large is-info"
                id="interpolation-slider-2"
                step="1" min="0" max="100" value="0" type="range">
      </div>
      <div class="column is-3 has-text-centered">
        <img src="./static/interpolated-sunflower/4.png"
              class="interpolation-image"
              alt="Interpolation end reference image."/>
        <p class="is-bold">End Frame</p>
      </div>
    </div> -->

  <!-- </div>
</section> -->

<!-- Method Overview. -->
<section class="section">
  <div class="container is-max-desktop" >
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <img src="./static/images/overview.png" class = "interpolation-image" alt="Method overview."/>
        <div class="content has-text-justified">
          We train a feed-forward text-to-3D diffusion model in two stages:
          <ul>

            <li>
              We train an unconditional GAN-based 3D object generator like <a herf="https://zj-dong.github.io/AG3D/">AG3D</a> with a single-view image dataset.
              3D objects are represented as triplanes and rendered with a renderer followed by an upsampler.
              We generate a large set of triplanes and caption them using <a herf="https://github.com/salesforce/BLIP">BLIP</a>. 
            </li>

            <li>
              The resulting (triplane, caption) dataset is then used to train a text-to-3D diffusion model,
              effectively distilling the GAN generator into a diffusion model, while also allowing for text conditioning.
              Triplanes generated by the text-to-3D model are rendered using the renderer and upsampler trained in the first stage.
            </li>

          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section" style="background-color:#f7f7f7">
<div class="columns is-centered">
  <div class="column is-full-width has-text-centered">
    <h2 class="title is-4">References</h2>

    <div class="content has-text-centered">
      <p>
        <a href="https://prior.allenai.org/projects/object-edit">Object 3DIT</a> is a language-guided model for 3D-aware image editing.
      </p>
      <p>
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_ObjectStitch_Object_Compositing_With_Diffusion_Model_CVPR_2023_paper.pdf">ObjectStitch</a> is a generative object compositing method which we adapt for object-level editing on scenes.
      </p>
      <p>
        <a href="https://zero123.cs.columbia.edu/">Zero-1-2-3</a> is a diffusion-based novel-view synthesis method which we use in conjunction with inpainting to perform object-level 3D-aware image editing.
      </p>
    </div>
  </div>
</div>
</section> -->

<!--/ Concurrent Work. -->


<!--<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->

<!-- 
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
          </p>
          <p>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

</body>
</html>
